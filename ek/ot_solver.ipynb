{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import ot\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train = np.load('../datasets/ratings_train.npy')\n",
    "ratings_test = np.load('../datasets/ratings_test.npy')\n",
    "m,n = ratings_train.shape\n",
    "data = np.nan_to_num(ratings_train)\n",
    "\n",
    "movies_average_rating = np.nanmean(ratings_train, axis=0)\n",
    "users_average_rating = np.nanmean(ratings_train, axis=1)\n",
    "movies_average_rating_normalized = movies_average_rating / movies_average_rating.sum()\n",
    "users_average_rating_normalized = users_average_rating / users_average_rating.sum()\n",
    "\n",
    "movies_total_rating = np.nansum(ratings_train, axis=0)\n",
    "users_total_rating = np.nansum(ratings_train, axis=1)\n",
    "users_normalized_total_rating = users_total_rating / users_total_rating.sum()\n",
    "movies_normalized_total_rating = movies_total_rating / movies_total_rating.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization(c,l):\n",
    "    return l*np.power(np.linalg.norm(c),2)/2\n",
    "\n",
    "def gradient_ridge_reg(c,c_hat,gamma,l):\n",
    "    return l*c + (c - c_hat)/gamma\n",
    "\n",
    "\n",
    "\n",
    "def compute_prox(c_hat, h_prox, x0, iterations = 100, gamma = 0.01, epsilon = 0.01,l = 0.01): #h is the non-differentiable function\n",
    "    xk = x0\n",
    "    gk = np.power(np.linalg.norm(xk-c_hat),2)/(2*gamma)\n",
    "    for _ in range(iterations):\n",
    "        xk_old = xk\n",
    "        # compute gradient for differentiable  part of function\n",
    "        gk_gradient = gradient_ridge_reg(xk,c_hat,gamma,l)\n",
    "        # take gradient step to reduce g(x)\n",
    "        xk_gradient = xk - gamma * gk_gradient\n",
    "        # proximal update to reduce h(x) but stay close to xk_gradient\n",
    "        xk = xk_gradient #Change if h is constraining function\n",
    "\n",
    "        if np.linalg.norm(xk - xk_old) < epsilon:\n",
    "            return xk\n",
    "\n",
    "    return xk\n",
    "\n",
    "def current_h(x,gamma):\n",
    "    return 0\n",
    "\n",
    "\n",
    "def cost_learning(pi_hat,mu,nu,epsilon,G, D, steps = 100):\n",
    "    m,n = pi_hat.shape\n",
    "    alpha = np.random.rand(m,1)\n",
    "    beta = np.random.rand(n,1)\n",
    "    u = np.exp(alpha / epsilon)\n",
    "    v = np.exp(beta / epsilon)\n",
    "    c = np.random.rand(m,n)\n",
    "    \n",
    "    G_inv = np.linalg.pinv(G)\n",
    "    D_inv = np.linalg.pinv(D)\n",
    "\n",
    "    A = np.random.rand(G_inv.shape[1], D_inv.shape[1])\n",
    "    \n",
    "    for i in range(steps):\n",
    "        K = np.exp(-c/epsilon)\n",
    "        kv = np.dot(K,v).reshape(m,1)\n",
    "        u = np.divide(mu.reshape(1,m),kv.reshape(1,m)).reshape(m,1)\n",
    "        ktu = np.dot(K.T,u).reshape(n,1)\n",
    "        v = np.divide(nu.reshape(1,n),ktu.reshape(1,n)).reshape(n,1)\n",
    "        K = pi_hat / (np.dot(u,v.T))\n",
    "        log_k = np.log(K, out=np.zeros_like(K), where=(K!=0))\n",
    "        A_deomposed = G_inv.T@log_k@D_inv\n",
    "        A = compute_prox(-epsilon*A_deomposed,current_h,A)\n",
    "        c = G.T@A@D\n",
    "\n",
    "    return epsilon*np.log(u),epsilon*np.log(v),c\n",
    "\n",
    "\n",
    "def transform_to_integers(matrix):\n",
    "    num_intervals = 10\n",
    "    min_value = matrix.min()\n",
    "    max_value = matrix.max()\n",
    "\n",
    "    # Calculate the interval width\n",
    "    interval_width = (max_value - min_value) / num_intervals\n",
    "\n",
    "    # Map the values to integers between 1 and 10\n",
    "    transformed_matrix = ((matrix - min_value) / interval_width).clip(0, num_intervals).astype(int) / 2\n",
    "\n",
    "    return transformed_matrix\n",
    "\n",
    "\n",
    "def compute_rmse(predictions, test_matrix):\n",
    "  masked = np.ma.array(test_matrix, mask=np.isnan(test_matrix))\n",
    "  diff = np.ma.subtract(predictions, masked)\n",
    "  squared = np.ma.power(diff, 2)\n",
    "  return np.ma.sqrt(np.ma.mean(squared))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "user_dist = pdist(data, metric='jaccard')\n",
    "user_sim_matrix = squareform(user_dist)\n",
    "\n",
    "movie_dist = pdist(data.T, metric='jaccard')\n",
    "movie_sim_matrix = squareform(movie_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ellington/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import scipy.sparse as sps\n",
    "\n",
    "csr_ratings_train = sps.csr_matrix(np.nan_to_num(ratings_train), shape=(610, 4980))\n",
    "csr_ratings_train.eliminate_zeros()\n",
    "\n",
    "nmf = NMF(n_components=10, init='nndsvd', random_state=0, max_iter=200)\n",
    "W = nmf.fit_transform(csr_ratings_train)\n",
    "H = nmf.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_hat = data / data.sum()\n",
    "\n",
    "mu_hat = pi_hat.sum(axis = 1)\n",
    "nu_hat = pi_hat.sum(axis = 0)\n",
    "\n",
    "G = W.T\n",
    "D = H\n",
    "\n",
    "lambd = 1e-3\n",
    "Gs = ot.sinkhorn(mu_hat, nu_hat, data, lambd)\n",
    "\n",
    "epsilon = 0.01\n",
    "\n",
    "alpha,beta,c = cost_learning(pi_hat, mu_hat, nu_hat, epsilon, G, D, steps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3037800\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1674037469201954\n",
      "1.1530560532408263\n"
     ]
    }
   ],
   "source": [
    "# otm = ot.emd(data.sum(axis=1), data.sum(axis=0), c)\n",
    "otm_scores = transform_to_integers(c)\n",
    "print(compute_rmse(otm_scores, ratings_train))\n",
    "print(compute_rmse(otm_scores, ratings_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
       " array([      7,      15,      42,     100,     143,     231,    1305,\n",
       "           1197, 3029263,    5496,       1]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(otm_scores, return_counts=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
